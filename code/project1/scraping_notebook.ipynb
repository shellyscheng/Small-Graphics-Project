{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium import webdriver\n",
    "\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Oyez Project Website\n",
    "\n",
    "## Selenium + BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=59.0.3071.115)\n  (Driver info: chromedriver=2.30.477690 (c53f4ad87510ee97b5c3425a14c0e79780cdf262),platform=Mac OS X 10.12.5 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f1ea5448077d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mpetitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/html/body/div/div/div[3]/main/article/div/aside/div[1]/div[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0moyez_case_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'petitioner'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpetitioner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PETITIONER'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    789\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    790\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    258\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'alert'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=59.0.3071.115)\n  (Driver info: chromedriver=2.30.477690 (c53f4ad87510ee97b5c3425a14c0e79780cdf262),platform=Mac OS X 10.12.5 x86_64)\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "oyez_list = []\n",
    "\n",
    "while a < 73:\n",
    "    oyez_case_dict = {}\n",
    "    \n",
    "    driver.get(\"https://www.oyez.org/cases/2016\")\n",
    "    \n",
    "    time.sleep(3)\n",
    "    granted = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/ng-include/ul/li['+ str(a) +']/aside/div[1]')\n",
    "    oyez_case_dict['granted'] = granted.text.replace('GRANTED', '').strip()\n",
    "    \n",
    "    argued = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/ng-include/ul/li['+ str(a) +']/aside/div[2]')\n",
    "    oyez_case_dict['argued'] = argued.text.replace('ARGUED', '').strip()\n",
    "    \n",
    "    decided = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/ng-include/ul/li['+ str(a) +']/aside/div[3]')\n",
    "    oyez_case_dict['decided'] = decided.text.replace('DECIDED', '').strip()\n",
    "    \n",
    "    summary = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/ng-include/ul/li['+ str(a) +']/div')\n",
    "    oyez_case_dict['summary'] = summary.text.strip()\n",
    "    \n",
    "    title = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/ng-include/ul/li['+ str(a) +']/h2/a')\n",
    "    title.click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    doc = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    cells = doc.find_all('div', attrs = {'class': 'row'})\n",
    "    \n",
    "    location = cells[0].find('div', attrs = {'class': 'cell ng-scope'})\n",
    "    if location:\n",
    "        location = location.a['iframe-url']\n",
    "        oyez_case_dict['location'] = re.findall(r'q=(.*)', location)[0]\n",
    "    \n",
    "    time.sleep(1)\n",
    "    petitioner = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/aside/div[1]/div[1]')\n",
    "    oyez_case_dict['petitioner'] = petitioner.text.replace('PETITIONER', '').strip()\n",
    "    \n",
    "    respondent = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/aside/div[1]/div[2]')\n",
    "    oyez_case_dict['respondent'] = respondent.text.replace('RESPONDENT', '').strip()\n",
    "    \n",
    "    docket = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/aside/div[2]/div[1]')\n",
    "    oyez_case_dict['docket'] = docket.text.replace('DOCKET NO.', '').strip()\n",
    "    \n",
    "    \n",
    "    time.sleep(1)\n",
    "    fact = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/section[1]/div')\n",
    "    oyez_case_dict['descriptions'] = fact.text.strip()\n",
    "\n",
    "    time.sleep(1)\n",
    "    conclusion = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/section[1]/div')\n",
    "    oyez_case_dict['conclusion'] = conclusion.text.strip()\n",
    "\n",
    "    oyez_list.append(oyez_case_dict)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = 1\n",
    "\n",
    "oyez_list2 = []\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "while b < 73:\n",
    "\n",
    "    oyez_case_dic = {}\n",
    "\n",
    "    driver.get(\"https://www.oyez.org/cases/2016\")\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "    title = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/ng-include/ul/li['+ str(b) +']/h2/a')\n",
    "    title.click()\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    doc = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "    maj_opin = doc.find('li', attrs = {'ng-if':'document.justia_opinion_url'})\n",
    "    if maj_opin:\n",
    "        oyez_case_dic['docket'] = maj_opin.a['href']\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    docket = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/aside/div[2]/div[1]')\n",
    "    oyez_case_dic['docket'] = docket.text.replace('DOCKET NO.', '').strip()\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        vote = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/section[3]/div[1]/div[3]/ul/li/figure/figcaption/h3')\n",
    "        oyez_case_dic['vote'] = vote.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        s_conclusion = driver.find_element_by_xpath('/html/body/div/div/div[3]/main/article/div/section[3]/div[1]/div[3]/ul/li/figure/figcaption/p')\n",
    "        oyez_case_dic['s_conclusion'] = s_conclusion.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    oyez_list2.append(oyez_case_dic)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_df2 = pd.DataFrame(oyez_list2)\n",
    "info_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame(oyez_list)\n",
    "info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Supreme Court Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx\")\n",
    "doc = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "caseno_list = doc.find_all('a')\n",
    "case_list = []\n",
    "case_dict = {}\n",
    "\n",
    "caseno_list = caseno_list[86:150]\n",
    "\n",
    "caseno_flist = []\n",
    "caseno_link = []\n",
    "file_name_list = []\n",
    "for case in caseno_list:\n",
    "    tran_link = case['href']\n",
    "    caseno_link.append(tran_link)\n",
    "    regex10 = r'[/]([0-9]{2}-[0-9]+_[0-9a-z]+).pdf$'\n",
    "    file_name = re.findall(regex10, tran_link)\n",
    "    for file in file_name:\n",
    "        file_name_list.append(file)\n",
    "    case = case.text.strip().replace('.','')\n",
    "    caseno_flist.append(case)\n",
    "    \n",
    "name_list = doc.find_all('span')\n",
    "name_list = name_list[18:82]\n",
    "name_flist = []\n",
    "for name in name_list:\n",
    "    name = name.text.strip()\n",
    "    name_flist.append(name)\n",
    "    \n",
    "    \n",
    "# date_list = doc.find_all('tr')\n",
    "# date_list = date_list[3:73]\n",
    "# date_flist = []\n",
    "# for date_cell in date_list:\n",
    "#     date = date_cell.find_all('td')[1].text.strip()\n",
    "#     if date != 'Date Argued':\n",
    "#         date_flist.append(date)\n",
    "        \n",
    "driver.close()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'docket': caseno_flist,\n",
    "    'caseName': name_flist,\n",
    "    #'date argued': date_flist,\n",
    "    'transcriptLink': caseno_link,\n",
    "    'fileName': file_name_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = df.merge(info_df, left_on=\"docket\", right_on=\"docket\")\n",
    "f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the oral arugment transcripts\n",
    "\n",
    "## regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scraping(sample_transcript):\n",
    "    trans_list = []\n",
    "    \n",
    "    \n",
    "    my_reg = r\"Alderson Reporting Company[^S]*Subject to Final Review\"\n",
    "    sample_transcript = re.sub(my_reg,\" \",sample_transcript)\n",
    "    \n",
    "    #sample_transcript = sample_transcript.replace('Alderson Reporting Company', '')\n",
    "    #sample_transcript = sample_transcript.replace('Official - Subject to Final Review', '')\n",
    "\n",
    "    regex = r\"\\b[12]{0,1}[0-9]?\\b\"\n",
    "    sample_transcript = re.sub(regex, '', sample_transcript)\n",
    "\n",
    "    chop_top = sample_transcript.split(\"PROCEEDINGS\")\n",
    "    sample_transcript = chop_top[1]\n",
    "\n",
    "    regex2 = r\"(submitted.[)])\"\n",
    "    chop_end = re.split(regex2,sample_transcript)\n",
    "    sample_transcript = chop_end[0] + chop_end[1]\n",
    "\n",
    "    regex3 = r'([A-Z]+[.]{0,1}[ ]{0,1}[A-Z]+\\W{0,2}[A-Z]+):'\n",
    "\n",
    "    transcript_list = re.split(regex3,sample_transcript)\n",
    "    transcript_list\n",
    "\n",
    "    transcript_list = transcript_list[1:]\n",
    "\n",
    "    d = 0\n",
    "    speaker_list = []\n",
    "    word_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for speaker in transcript_list:\n",
    "        if d % 2 == 0:\n",
    "            speaker_list.append(speaker.strip())\n",
    "        else:\n",
    "            word_list.append(speaker.strip())\n",
    "        d = d + 1\n",
    "    \n",
    "    for item1,item2 in zip(speaker_list, word_list):\n",
    "        trans_dict = {}\n",
    "        trans_dict[\"speaker\"] = item1\n",
    "        trans_dict[\"word\"] = item2\n",
    "        trans_list.append(trans_dict)\n",
    "        \n",
    "    return trans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_trans_list = []\n",
    "z = 0\n",
    "\n",
    "for file, case in zip(file_name_list, caseno_flist):\n",
    "    f = open('/Users/Shiying/Documents/ColumbiaLede/data_database/supreme_court/pdfs/' + file + '.txt', 'rb')\n",
    "    \n",
    "    try:\n",
    "        #print('/Users/Shiying/Documents/ColumbiaLede/data_database/supreme_court/pdfs/' + file + '.txt')\n",
    "        this_transcript = f.read().decode('utf8', 'ignore')\n",
    "        trans_list = scraping(this_transcript)\n",
    "        for item in trans_list:\n",
    "            item['docket'] = case\n",
    "        \n",
    "        all_trans_list.extend(trans_list)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "df2 = pd.DataFrame(all_trans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df2[df2['speaker'].str.contains('JUSTICE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speak_lines = df3.sort_values(by='speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines_out = speak_lines.groupby(['docket','speaker'])['word'].apply(lambda x: \"<p>%s</p>\" % '</p><p> '.join(x)).reset_index(name='lines')\n",
    "lines_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines_out['line_dict'] = lines_out.apply(lambda row: {row['speaker']:row['lines']}, axis=1)\n",
    "lines_out.to_csv('line_out.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for item in lines_out['docket'].unique():\n",
    "    result = {}\n",
    "    for x in lines_out[lines_out['docket'] == item]['line_dict']:\n",
    "        result.update(x)\n",
    "    result_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_out.apply(lambda x: \"%s\" % '</p><p> '.join(x)).reset_index(name='lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df4 = df3.groupby('docket')['speaker'].value_counts().reset_index(name = 'per_docket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df4['string'] = df4['speaker'].str.title() + \": \" + df4['per_docket'].map(str)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = df4.groupby('docket')['string'].apply(lambda x: \",  \".join(x)).reset_index(name='article')\n",
    "output2 = df4.groupby('docket')['string'].apply(lambda x: \"<p>%s</p>\" % '</p><p> '.join(x)).reset_index(name='article')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output['f_speaker_line'] = result_list\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "justices = df3.groupby('speaker')['docket'].value_counts().reset_index(name='times')\n",
    "justices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "justices2 = output2\n",
    "justices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices2['speak_in'] = '0'\n",
    "justices2['color'] = 'CE21C3'\n",
    "justices2 = justices2[['docket', 'speak_in', 'color']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "justices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "justices.loc[justices.speaker == 'CHIEF JUSTICE ROBERTS', 'speak_in'] = '1'\n",
    "justices.loc[justices.speaker == 'JUSTICE ALITO', 'speak_in'] = '2'\n",
    "justices.loc[justices.speaker == 'JUSTICE BREYER', 'speak_in'] = '3'\n",
    "justices.loc[justices.speaker == 'JUSTICE GINSBURG', 'speak_in'] = '4'\n",
    "justices.loc[justices.speaker == 'JUSTICE GORSUCH', 'speak_in'] = '5'\n",
    "justices.loc[justices.speaker == 'JUSTICE KAGAN', 'speak_in'] = '6'\n",
    "justices.loc[justices.speaker == 'JUSTICE KENNEDY', 'speak_in'] = '7'\n",
    "justices.loc[justices.speaker == 'JUSTICE SOTOMAYOR', 'speak_in'] = '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speak_perdocket = justices.groupby('docket')['speak_in'].value_counts().reset_index(name='times')\n",
    "speak_perdocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speak_color = justices.groupby('docket')['color'].value_counts().reset_index(name='times')\n",
    "speak_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_speak_perdocket = pd.concat([speak_perdocket, justices2])\n",
    "f_speak_perdocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_speak_color = pd.concat([speak_color, justices2])\n",
    "f_speak_color.groupby('docket')['color'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = f_speak_perdocket.groupby('docket')['speak_in'].apply(lambda x: ','.join(x).split(',')).reset_index(name='speak_list')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a['color'] = \"['CE21C3', '0BB2E8','37CB00','8A04D8','D82C86','0BD488','000000','7E3B0D']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Merge all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_df = f_df.merge(output, left_on = 'docket', right_on = 'docket')\n",
    "f_df = f_df.merge(a, left_on = 'docket', right_on = 'docket')\n",
    "f_df = f_df.merge(info_df2, left_on = 'docket', right_on = 'docket')\n",
    "f_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn the longtitude and latitude into geopandas points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_df['lat'] = f_df['location'].apply(lambda x: x.split(',')[0])\n",
    "f_df['lon'] = f_df['location'].apply(lambda x: x.split(',')[1])\n",
    "f_df['lat'] = f_df['lat'].astype(float)\n",
    "f_df['lon'] = f_df['lon'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_df = f_df[['caseName', 'docket', 'granted', 'argued', 'decided', 'lat', 'lon', 'summary', 'vote', 's_conclusion','f_speaker_line', 'descriptions', 'conclusion', 'article', 'speak_list', 'color']]\n",
    "ff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gf_df = gpd.GeoDataFrame(ff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gf_df['geometry'] = gf_df.apply(lambda row: Point(row.lon, row.lat), axis=1)\n",
    "gf_df['speak_list'] = gf_df.speak_list.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gf_df.to_file('case500.json', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
